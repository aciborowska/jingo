I have no idea at the moment if this is feasible or not, but here is the idea.

When running Pig in production it happens quite frequently that the building of the plan takes a fair amount of time compared to the actual execution of the job on the Hadoop cluster.

For jobs which are run periodically without modification of the underlying pig script it would be nice to be able to store the execution plan when it is first built, then load that persisted plan for all other executions.

A unique signature of the script could be computed, for example by removing comments and blank lines and computing a digest, and the computed plan stored under that name with a signature mechanism to ensure it's correct.

This would speed up lots of executions of Pig.