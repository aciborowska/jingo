pig.maxCombinedSplitSize defaults to block size. If there are lot of small bz files which will uncompress to big data, they were combined till the block size was reached which was 128 MB in our case. The load took 20 mins, but using pig.noSplitCombination=true cut down the time to 2+mins. 

Need intelligent logic to take into account the factor the input split will expand to when uncompressed (factor will differ for different compression formats like bz and gz and can be configurable by user) and use the expanded size as an estimate while combining splits.