A table created by HCAT has the following partitions; 

hcat -e "show partitions paritionedtable"


grid=AB/dt=2011_07_01
grid=AB/dt=2011_07_02
grid=AB/dt=2011_07_03
grid=XY/dt=2011_07_01
grid=XY/dt=2011_07_02
grid=XY/dt=2011_07_03
grid=XY/dt=2011_07_04
...

The total number of partitions in the table is around 3200.

A Pig script of this nature tries to access this data using the partitions in it's filter. 

{script}
A = LOAD 'paritionedtable' USING org.apache.hcatalog.pig.HCatLoader();
B = FILTER A BY grid=='AB' AND dt=='2011_07_04';
C = LIMIT B 10;
store C into 'HCAT' using PigStorage();{script}


This script, fails to run as the job.xml generated by Pig is so large (8MB), that the Hadoop Fred's limitation does not allow it to submit the job. 

After debugging it was found that in the HCatTableInfo class the function gets a null filter value. getInputTableInfo(filter=null ..)

I suspect that "setPartitionFilter" function in Pig does not pass the filter correctly to the HCatLoader. This is happening with both Pig 0.9 and 0.8

Viraj