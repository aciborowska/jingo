The current implementation of Netty enable/disable recv logic may cause the direct buffer OOM because we may enable read a large chunk of packets and disabled again after consuming a single ZK request. We have seen this problem on prod occasionally.
 
Need a more advanced flow control in Netty instead of using AUTO_READ. Have improved it internally by enable/disable recv based on the queuedBuffer size, will upstream this soon.
 
With this implementation, the max Netty queued buffer size (direct memory usage) will be 2 * recv_buffer size. It's not the per message size because in epoll ET mode it will try to read until the socket is empty, and because of SslHandler will trigger another read when it's not a full encrypt packet and haven't issued any decrypt message.