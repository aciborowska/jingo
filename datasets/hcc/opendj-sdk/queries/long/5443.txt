There is already an existing mechanism in changelog to recover corrupted files but it has proven insufficient with some issue found by the Saas team.
 If server is not stopped gracefully, it is possible to have a changelog file that contains a high number of zero (00) bytes at the end of the file. This happens only to the writable file, and is probably due to the file system updating the metadata of the file (i.e increasing its size) but not having time to actually write the data in the file.

There are at least three things to improve:

	Do not read only records after the last block, because the last block can be located in the "zero byte" section. There are two options: either read all records from the beginning of the file, or start by the last block of the file and go backward until some valid record is found. (May be obsoleted by OPENDJ-5076)
	When reading size of the next record, consider it can be equal to zero if the "zero byte" section is reached. In that case, consider the log is corrupted from this point
	In case of corruption, the recovery is done by truncating the file after the last valid record. The current way it is done by truncating the file directly is not correct, according to the Theodore Ts'o (linux file system expert):
 "There is a similar hack so that if an existing file is truncated using O_TRUNC at open time, when that file is closed, we will also initiate an implicit writeback. Even with ext3, if you crash at the wrong time, it's possible for the open with O_TRUNC to delete all of the data in the existing file, and for the modified data to be lost after a crash, so I don't recommend that application programmers read from an existing file, and then rewrite it by truncating the file and then rewriting the data. The only safe, portable way to update an existing file is to write the modified contents to "foo.new", call fsync(2) on the file, then rename "foo.new" to "foo", checking the error returns of the system call at each step."

