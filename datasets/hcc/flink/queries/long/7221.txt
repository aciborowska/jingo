I have a data set with ~17000 rows that I was trying to write to a PostgreSQL table that I did not (yet) have permission on. No data was loaded, and Flink did not report any problem outputting the data set. The only indication I found of my problem was in the PostgreSQL log.

With the default parallelism (8) and the default batch interval (5000), my batches were ~2000 rows each, so they were never executed in JDBCOutputFormat.writeRecord(..). JDBCOutputFormat.close() does a final call on upload.executeBatch(), but if there is a problem, it is logged at INFO level and not rethrown. 

If I decrease the batch interval to 100 or 1000, then an error is properly reported.