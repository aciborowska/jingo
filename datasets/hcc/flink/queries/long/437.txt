I picked up the code of https://github.com/stratosphere/stratosphere/pull/424 and enhanced it with the following:


	make hadoop-compat compatible to hadoop yarn and remove code in package `org.apache.hadoop`.
	Extended usercode object wrapper (@aljoscha, please validate my changes), fixed InputFormat serialization
	introduce pluggable type converter
	Add a generic wrapper for Hadoop's `Writable` and `WritableComparable`.



The generic wrapper allows to do the following: (The example is from Mongodb's Hadoop InputFormat)
```java
public void map(Record record, Collector<Record> out) throws Exception {
	WritableWrapper wrap = record.getField(0, WritableWrapper.class);
	Writable wr = wrap.value();
	Writable valWr = record.getField(1, WritableWrapper.class).value();
	BSONWritable bson = (BSONWritable) wr;
	BSONWritable value = (BSONWritable) valWr;
	System.err.println("bson value has "+value.toString());
}
```

Lets discuss if we want the project being called "hadoop-compat" or if you prefer "hadoop-compatability" ?

Open issues:

	If the Hadoop IF is a FileInputFormat, we should make the split assignment file locality aware
	Add junit tests for various formats such as ORC, Parquet and Avro.
	Write website documentation for this code.
	Add HadoopDataSink
	See if it is possible to map hadoop counters to our accumulators



---------------- Imported from GitHub ----------------
Url: https://github.com/stratosphere/stratosphere/pull/437
Created by: rmetzger
Labels: 
Milestone: Release 0.5
Created at: Sat Jan 25 13:54:15 CET 2014
State: closed