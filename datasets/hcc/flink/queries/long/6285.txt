As of Flink 1.2.0, the binary distribution does not include classes from the hadoop-compatibility dependency anymore.


flink-1.2.0> for i in lib/*.jar; jar tf $i | grep WritableTypeInfo; end
flink-1.2.0 [1]> # the above finds nothing



Therefore, it is necessary to copy the compatibility jar to flink's installation `lib/` directory (or a sub-directory) if one wishes to use hadoop input formats. Merely packaging the compatibility jar as part of an application's "fat jar" does not suffice, as code in TypeExtractor#createHadoopWritableTypeInfo's relies on being able to see the compatibility classes through the classloader TypeExtractor itself was loaded by. On yarn this seems not to be the case (e.g. when running the application through flink run -m yarn-cluster ...).

Ideally, we'd fix the class loading issue, such that flink's installation does not need to be altered, due to the need of a particular application. Alternatively, we could include the hadoop-compatibility jar as part of the binary distribution and provide corresponding instructions, 1 and 2 seem to be good places.