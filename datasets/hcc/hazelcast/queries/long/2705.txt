Reproduced with Hazelcast 3.3.EA and latest maintenance-3.x of today.

If there is another machine running in the network with an older hazelcast instance (3.1.5 in my case), then certain multicast packets can lead to an overflow in integer parsing inside com.hazelcast.nio.UTFEncoderDecoder#readUTF0() which then tries to allocate huge arrays.

This other machine is neither in the same cluster, nor has the same version and thus this should exit early and not being tried to get parsed and read.

At least there should be some logic check inside UTFEncoderDecoder whether the length read from stream is bigger than the actual data of the ByteArrayObjectDataInput - this then already indicates an error.

Please see screenshot below and also a dump of the multicast-data, which leads to the OOM is available here: http://euve34545.vserver.de/temp/Buffer.dump

![hz-oom](https://cloud.githubusercontent.com/assets/475403/3256670/e2394844-f21b-11e3-8ad4-0b34938d5a5b.png)
