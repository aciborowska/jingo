_Issue created according to feedback from a user:_

**Version: 3.3.4**

> During stress testing of controllers we have noticed that hazelcast is incorrectly merging the partitions.
> 1. Due to network load (no artificial partitions between the hazelcast instances), one of the controller got dropped from the 3 node cluster - Say C got disconnected from A & B.
> 2. A & B realize that C is gone and become a stable 2 node cluster.
> 3. At this time, C detected A is gone, it still has to detect that B is gone. So C thinks it is a cluster with 2 nodes (B & C).
> 4. Now the TCP connections get reestablished and the cluster â€“ A B merges to B C (Which should have been just C).
> 5. During this merge process, C detects that B is gone and then we restart the HZ instance on C (we restart the minority as designed).
> 6. Once HZ instance on C is restarted, the new HZ instance on C joins the cluster with A & B, and we have a 3 node cluster A B C again.
> 
> However, we continue to see the following exceptions on two controllers with Locks even after 30 minutes of partition getting fixed.
> 
> [2015-02-23 15:35:26.330] WARN partition-operation.thread-6 com.hazelcast.spi.impl.WaitNotifyServiceImpl$WaitingOp DE0005I [NODE1]:5700 [dev] [3.3.4] Op: com.hazelcast.concurrent.lock.operations.LockOperation@249a2ce6, com.hazelcast.spi.exception.WrongTargetException: WrongTarget! this:Address[NODE1]:5700, target:Address[NODE2]:5700, partitionId: 110, replicaIndex: 0, operation: com.hazelcast.spi.impl.WaitNotifyServiceImpl$WaitingOp, service: hz:impl:lockService
