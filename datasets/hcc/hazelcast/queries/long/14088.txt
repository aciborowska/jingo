When the cluster has some data (tested with ~1GB) and doing a rolling upgrade one by one, sometimes when the last member gets restarted, it fails with the following error.

```
Nov 07, 2018 8:41:27 AM com.hazelcast.internal.cluster.impl.DiscoveryJoiner
INFO: [10.16.0.19]:5701 [dev] [3.10.5] [10.16.0.15]:5701 is added to the blacklist.
Nov 07, 2018 8:41:27 AM com.hazelcast.nio.tcp.TcpIpConnectionErrorHandler
WARNING: [10.16.0.19]:5701 [dev] [3.10.5] Removing connection to endpoint [10.16.0.15]:5701 Cause => java.net.SocketException {Host is unreachable to address /10.16.0.15:5701}, Error-Count: 13
Nov 07, 2018 8:41:27 AM com.hazelcast.internal.partition.impl.PartitionReplicaStateChecker
OFF: [10.16.0.19]:5701 [dev] [3.10.5] Could not get a response from master about migrations! -> com.hazelcast.spi.exception.TargetNotMemberException: Not Member! target: [10.16.0.15]:5701, partitionId: -1, operation: com.hazelcast.internal.partition.operation.HasOngoingMigration, service: hz:core:partitionService
```

Looks like maybe some ongoing migrations have the address fixed and the member cannot connect. Or something with master election. Maybe just a simple retry on the members discovery would solve the issue.

The easier to reproduce is to do it on Kubernetes (then, usually all PODs are assigned with new IPs). However, I believe, the issue is not limited to Kubernetes.

Steps to reproduce on Kubernetes (using Helm Chart):
1. Start a Hazelcast Cluster
```
helm install --name my-release --set cluster.memberCount=6,image.tag=3.10.3 stable/hazelcast
```
2. Insert some data (can skip this point, but the issue happens less frequently without any data)
3. Upgrade the cluster
```
helm upgrade my-release --set cluster.memberCount=6,image.tag=3.10.4 stable/hazelcast
```