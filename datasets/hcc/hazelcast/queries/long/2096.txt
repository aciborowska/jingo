Hi,

remember the issue #2027 ? This is fixed now thanks to pull request #2041 . 

Unfortunately, a similar problem can still occur during partition migration at runtime (e.g. when a new member joins). It seams that if the map.put() call falls into one write-behind interval and the map.remove() call into the next one, plus the entry is migrated from one member to another, then the MapStore.delete() operations is triggered before the MapStore.store() operation - actually even several seconds before?!

Furthermore, during partition migration, it may also happen that map.remove() calls do _not_ result in MapStore.delete() calls.

I tested it using version 3.2. The following test shows both problems. Look at the left-over entries at the end of the test output and search for those keys in the produced output.

``` java
package com.nm.test.hazelcast;

import com.hazelcast.config.Config;
import com.hazelcast.config.MapConfig;
import com.hazelcast.config.MapStoreConfig;
import com.hazelcast.config.XmlConfigBuilder;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.IMap;
import com.hazelcast.core.MapStore;
import org.apache.log4j.BasicConfigurator;
import org.apache.log4j.Logger;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import junit.framework.TestCase;

public class TestMapStore4 extends TestCase {

    private static final Logger logger = Logger.getLogger(TestMapStore4.class);

    private static final String mapName = "testMap";

    private static final int numIterations = 120;

    private static final int blockSize = 6;

    private static final int writeDelaySeconds = 10;

    private static final boolean simulateSecondNode = true;

    public static void main(String[] args) throws Exception {

        // configure logging
        BasicConfigurator.configure();

        // run test
        TestMapStore4 test = new TestMapStore4();
        test.testMapStore();
    }

    public void testMapStore() throws Exception {

        // create shared hazelcast config
        final Config config = new XmlConfigBuilder().build();
        config.setProperty("hazelcast.logging.type", "log4j");

        // create shared map store implementation
        RecordingMapStore store = new RecordingMapStore();

        // configure map store
        MapStoreConfig mapStoreConfig = new MapStoreConfig();
        mapStoreConfig.setEnabled(true);
        mapStoreConfig.setWriteDelaySeconds(writeDelaySeconds);
        mapStoreConfig.setClassName(null);
        mapStoreConfig.setImplementation(store);
        MapConfig mapConfig = config.getMapConfig(mapName);
        mapConfig.setMapStoreConfig(mapStoreConfig);

        // thread 1:
        // add and remove entries
        Thread thread1 = new Thread(new Runnable() {

            @Override
            public void run() {
                HazelcastInstance hcInstance = Hazelcast.newHazelcastInstance(config);

                // try-finally to stop hazelcast instance
                try {

                    // log begin put/remove
                    logger.info(Thread.currentThread().getName() + " starting put/remove...");

                    // loop over num iterations
                    // 120 iterations * 250ms -> around 30s
                    IMap<String, String> map = hcInstance.getMap(mapName);
                    for (int k = 0; k < numIterations; k++) {
                        final String blockKeyPrefix = String.valueOf(k + 100); // 3 digits for sorting in output

                        // add a block of entries
                        // since we add several entries,
                        // they will fall into different partitions
                        for (int j = 0; j < blockSize; j++) {
                            String key = blockKeyPrefix + j;
                            String value = "v:" + key;
                            map.put(key, value);
                        }

                        // sleep 250ms
                        sleep(250, false);

                        // remove the entries again
                        // since we slept 250ms before removing the entries again,
                        // the store and remove calls might fall into different write-behind intervals
                        for (int j = 0; j < blockSize; j++) {
                            String key = blockKeyPrefix + j;
                            map.remove(key);
                        }
                    }

                    // log end put/remove
                    logger.info(Thread.currentThread().getName() + " put/remove done.");

                    // sleep 30s
                    // 5s join + 30s working + 30s sleeping -> around 65s
                    sleep(30000, true);

                } finally {
                    hcInstance.getLifecycleService().shutdown();
                }
                logger.info(Thread.currentThread().getName() + " done.");
            }
        }, "Thread 1");
        thread1.start();

        // wait 15s after starting first thread
        sleep(15000, true);

        // thread 2:
        // simulate a second member which joins the cluster
        Thread thread2 = new Thread(new Runnable() {

            @Override
            public void run() {
                HazelcastInstance hcInstance = Hazelcast.newHazelcastInstance(config);

                // log joined
                logger.info(Thread.currentThread().getName() + " hazelcast instance joined.");

                // try-finally to stop hazelcast instance
                try {

                    // sleep 45s
                    // -> 15 waiting before join + 5s join + 45 -> around 65s
                    sleep(45000, true);

                } finally {
                    hcInstance.getLifecycleService().shutdown();
                }
                logger.info(Thread.currentThread().getName() + " done.");
            }
        }, "Thread 2");
        if (simulateSecondNode) {
            thread2.start();
        }

        // join threads
        thread1.join();
        if (simulateSecondNode) {
            thread2.join();
        }

        // print store content
        TreeSet<String> setSorted = new TreeSet<String>(store.getStore().keySet());
        logger.info("Store: " + setSorted);
        if (!setSorted.isEmpty()) {
            fail("Left-over entries exist: " + setSorted);
        }
    }

    private static void sleep(long ms, boolean log) {
        try {
            Thread.sleep(ms);
            if (log) {
                logger.info("Slept " + (ms / 1000) + "s.");
            }
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }
    }

    public static class RecordingMapStore implements MapStore<String, String> {

        private static final Logger logger = Logger.getLogger(RecordingMapStore.class);

        private ConcurrentHashMap<String, String> store = new ConcurrentHashMap<String, String>();

        public ConcurrentHashMap<String, String> getStore() {
            return store;
        }

        @Override
        public String load(String key) {
            //logger.info("load(" + key + ") called.");
            return store.get(key);
        }

        @Override
        public Map<String, String> loadAll(Collection<String> keys) {
            List<String> keysList = new ArrayList<String>(keys);
            Collections.sort(keysList);
            logger.info("loadAll(" + keysList + ") called.");
            Map<String, String> result = new HashMap<String, String>();
            for (String key : keys) {
                String value = store.get(key);
                if (value != null) {
                    result.put(key, value);
                }
            }
            return result;
        }

        @Override
        public Set<String> loadAllKeys() {
            logger.info("loadAllKeys() called.");
            Set<String> result = new HashSet<String>(store.keySet());
            logger.info("loadAllKeys result = " + result);
            return result;
        }

        @Override
        public void store(String key, String value) {
            logger.info("store(" + key + ") called.");
            String valuePrev = store.put(key, value);
            if (valuePrev != null) {
                logger.warn("- Unexpected Update (operations reordered?): " + key);
            }
        }

        @Override
        public void storeAll(Map<String, String> map) {
            TreeSet<String> setSorted = new TreeSet<String>(map.keySet());
            logger.info("storeAll(" + setSorted + ") called.");
            store.putAll(map);
        }

        @Override
        public void delete(String key) {
            logger.info("delete(" + key + ") called.");
            String valuePrev = store.remove(key);
            if (valuePrev == null) {
                logger.warn("- Unnecessary delete (operations reordered?): " + key);
            }
        }

        @Override
        public void deleteAll(Collection<String> keys) {
            List<String> keysList = new ArrayList<String>(keys);
            Collections.sort(keysList);
            logger.info("deleteAll(" + keysList + ") called.");
            for (String key : keys) {
                String valuePrev = store.remove(key);
                if (valuePrev == null) {
                    logger.warn("- Unnecessary delete (operations reordered?): " + key);
                }
            }
        }

    }

}
```

Ps. I think this is a different issue then #2095 , however, they might be related...

Thanks and best,
Lukas
