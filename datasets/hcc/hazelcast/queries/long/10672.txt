HZ 3.8.2, single node deploy, -Xmx=4G. In one of my tests I populate caches with entities and then perform queries against this data. Usually the whole memory allocated for my test data set is ~700M. On the second test phase some temporary data structures created, but then minor-GCed quickly. Now I specified a custom attribute with extractor and an index on this attribute in one of my caches:
```
<hz:map name="xdm-result" in-memory-format="BINARY" async-backup-count="${bdb.schema.query.backup.async}" 
		 backup-count="${bdb.schema.query.backup.sync}" read-backup-data="${bdb.schema.query.backup.read}"
		 eviction-policy="LRU" time-to-live-seconds="0" max-size-policy="USED_HEAP_PERCENTAGE" 
		 max-size="10">
	<hz:indexes>
		<hz:index attribute="docId"/>
	</hz:indexes>
	<hz:attributes>
		<hz:attribute name="docId" extractor="com.bagri.server.hazelcast.predicate.ResultsQueryExtractor" />
	</hz:attributes>
</hz:map>
```
This cache is actively used on the second (query) test phase. Now the first population phase takes usual time and allocates the same amount of memory. But then on the second phase I see abnormal memory allocation activity, in about a minute it alocates the whole 4G and after that I see periodical major GC every ~10 sec. Investigating memory dump I see a lot of int[] (1.7G) and Object[] (1G) allocated, which are not allocated usually. 

The cache with custom attribute has 41666 enities and pass the same amount of keys to collector. So it is not clear why that custom index takes so much memory.

I do not have a reproducer test, but I can explain how to run this scenario with my project quickly. Please have a look.

Thanks, Denis.