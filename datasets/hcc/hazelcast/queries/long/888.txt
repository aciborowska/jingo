I am running two process that are setup as a hazelcast cluster.  Other processes connect to this cluster to request data.  Data is set with backup quantity set to 1.  We are using hazelcast 2.1.2.

We are observing the following behavior:
- All hazelcastclient processes connect to the first node int he cluster by default.  
- Each cluster starts at about 3G of memory
- The memory on the first hazelcast node grows slowly until it hits our 8G max JVM setting and then hangs after throwing an OOM
- hazelcastclients that are calling getMap(<mapname>).values() start having CONCURRENT_MAP_ITERATE_ENTRIES timeouts
- We kill the first hazelcast node and all client connect to the second node. The memory there slowly grows until it too throws OOM.

From reading the docs values() should return a collection to the calling process so we are wondering why we would be getting concurrent map iterate entries happening.  We are not using any predicates to search or cause the cache node to iterate.

In addition we are curious as to what's causing the memory growth and crash.  Our thought is that it could be the cache node making all the duplicate copies of data to pass back to the clients calling values().

Any thoughts?
