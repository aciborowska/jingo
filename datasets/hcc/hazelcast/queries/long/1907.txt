I have a map configured with non-zero "time-to-live-seconds" and "max-idle-seconds" (tried 300 seconds for both). This map has high update rate (hundreds of updates per second), but relatively small number of objects (~5000). I noticed that memory consumption is much higher than I expected, so I used a profiler and found that  most space is taken by SecondsBasedEntryTaskScheduler with entry processor equal to EvictionProcessor. 

Looks like on any map change, an eviction task is scheduled (and never deleted from task queue until its execution time arrives). So when there are lots of updates per second, summary size of all eviction tasks for map entry can be much larger than the entry itself. In fact, eviction task queue can grow until it consumes all VM memory, even if there are few records in the map. I think this behavior is bad. Ideally, only fixed number of eviction tasks should exist for each map entry. Or at least we should have a way to limit eviction task queue size.
