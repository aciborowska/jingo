**Description**
We are losing data from distributed map in the event of a node failure in 3 node cluster with backup count set to 1. This is due to the use of the lite members and promoting them to data member.

**To Reproduce**

Steps to reproduce the behavior:
1. Start the cluster with 3 nodes - **all nodes are configured as lite members**
2. Set backup count for the map to 1
3. Wait until cluster is up and running
4. Promote all members to data members - calling `hzlInstance.cluster.promoteLocalLiteMember()`
5. Insert 1000 records into the distributed map
6. Kill one node - `hzlInstance.lifecycleService.terminate()`
7. Count number of records in the distributed map

**Actual result:**
Data is lost even with backup count set to 1. 
There is not 1000 records in the map.

**Expected behavior:**
No data is lost after one node is down. There will be 1000 records in the map.

**Additional context**

- This happens on Hazelcast version 4.0.3.
- Works as expected on version 3.10.2.
- Java 11
- Windows

**Failing unit test written in Kotlin:**
```Kotlin
import com.hazelcast.config.Config
import com.hazelcast.config.MergePolicyConfig
import com.hazelcast.core.Hazelcast
import com.hazelcast.spi.merge.DiscardMergePolicy
import org.junit.jupiter.api.Test
import kotlin.test.assertEquals
import kotlin.test.assertTrue

class HazelcastTest {
    @Test
    fun `hazelcast bug unit test`() {
        val numberOfRecordsInsertedIntoMap = 1000

        // start first hazelcast instance as lite member
        val firstHazelcastInstance = Hazelcast.newHazelcastInstance(createConfig(5701))
        Thread.sleep(10000)
        assertTrue(firstHazelcastInstance.lifecycleService.isRunning)

        // start second and third hazelcast instances as lite member
        val secondHazelcastInstance = Hazelcast.newHazelcastInstance(createConfig(5702))
        val thirdHazelcastInstance = Hazelcast.newHazelcastInstance(createConfig(5703))
        Thread.sleep(10000)
        assertTrue(secondHazelcastInstance.lifecycleService.isRunning)
        assertTrue(thirdHazelcastInstance.lifecycleService.isRunning)

        // promote all instances to data members
        firstHazelcastInstance.cluster.promoteLocalLiteMember()
        secondHazelcastInstance.cluster.promoteLocalLiteMember()
        thirdHazelcastInstance.cluster.promoteLocalLiteMember()


        // give cluster some time
        Thread.sleep(5000)

        // check if cluster is in good shape
        assertTrue(firstHazelcastInstance.partitionService.isClusterSafe)

        // insert some dummy data into the testing map
        val testMap = firstHazelcastInstance.getMap<String, String>("test-map")
        for (i in 1..numberOfRecordsInsertedIntoMap) {
            testMap["key$i"] = "value$i"
        }

        // check all data is correctly inserted
        assertEquals(numberOfRecordsInsertedIntoMap, testMap.size)

        // kill second instance (simulate node down)
        secondHazelcastInstance.lifecycleService.terminate()

        // wait
        Thread.sleep(2000)

        // backup count for the map is set to 1
        // even with 1 node down no data loss is expected
        assertEquals(numberOfRecordsInsertedIntoMap, firstHazelcastInstance.getMap<String, String>("test-map").size)
        assertEquals(numberOfRecordsInsertedIntoMap, thirdHazelcastInstance.getMap<String, String>("test-map").size)
    }

    private fun createConfig(port: Int): Config {
        val config = Config()

        // start instance as lite member
        config.isLiteMember = true

        // cluster name
        config.clusterName = "TestClusterName"

        // network configuration
        config.networkConfig.port = port
        config.networkConfig.isPortAutoIncrement = false
        config.networkConfig.isReuseAddress = true
        config.networkConfig.restApiConfig.isEnabled = false

        config.networkConfig.join.multicastConfig.isEnabled = false

        config.networkConfig.join.tcpIpConfig.isEnabled = true
        config.networkConfig.join.tcpIpConfig.addMember("127.0.0.1:5701")
        config.networkConfig.join.tcpIpConfig.addMember("127.0.0.1:5702")
        config.networkConfig.join.tcpIpConfig.addMember("127.0.0.1:5703")

        // map configuration
        val defaultMapConfig = config.getMapConfig("default")
        defaultMapConfig.backupCount = 1
        defaultMapConfig.mergePolicyConfig = MergePolicyConfig(DiscardMergePolicy::class.java.canonicalName, MergePolicyConfig.DEFAULT_BATCH_SIZE)
        config.addMapConfig(defaultMapConfig)

        return config
    }
}
```

_We are using lite members to count members in the cluster and then we have some logic if we want to become a full data member or not._
