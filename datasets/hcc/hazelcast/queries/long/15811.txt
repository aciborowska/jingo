We are facing a severe OutOfMemory leak due to hazelcast library. We are using Hazelcast **IMap** as distributed cache for storing and processing objects in multi-node cluster setup. After processing is finished, I am calling **IMap.remove(String key)** method to remove the object. Even though this method removes the entry from map, but I can still see the corresponding objects in heapdump. These objects are never cleaned (even after waiting for 5 days) and accumulate overtime and causing OutOfMemory error. Some additional information on setup and for repro :

1. Initially issue occurred on hazelcast 3.7.1, but still persists after upgrade to 3.12.3.
2. It is a 2 node cluster setup
3. Java version 1.8.0_91
4. Operating system : Red Hat Enterprise Linux Server 7.0 (Maipo) (kernal : 3.10.0-123.el7.x86_64)
5. IMap configuration
```
    <map name="workflow_status">
        <in-memory-format>BINARY</in-memory-format>
        <time-to-live-seconds>86400</time-to-live-seconds>
        <max-idle-seconds>86400</max-idle-seconds>
        <eviction-policy>NONE</eviction-policy>
        <max-size policy="PER_NODE">0</max-size>
        <merge-policy>com.hazelcast.map.merge.PassThroughMergePolicy</merge-policy>
    </map>
```
6. Attached a sample java program to reproduce the issue [HazelcastTester.txt](https://github.com/hazelcast/hazelcast/files/3742463/HazelcastTester.txt)
7. Attached sample data used in above program [loadWorkflows.txt](https://github.com/hazelcast/hazelcast/files/3742462/loadWorkflows.txt)
8. A removed object still present in heapdump after 2 days 
<img width="704" alt="deleteObjectInHeap" src="https://user-images.githubusercontent.com/2444254/67069262-233d7a80-f19a-11e9-9af8-e3e76d4bb7b8.png">

I would be happy to provide any additional information, if required
