```
I implement MapStore for Persistence.I started 2 node Hazelcast,I put 500000 data to Map,I found The first node hold 240000 data and the second node hold 260000 data,At this time no data lost.
When Stop all node and later restart all node,I found the Map size less than 500000,data lost.
After several tests, the results are so!!!
```

my source code:

<pre><span>
package org.hazelcast.server.persistence;

import java.io.File;
import java.util.Collection;
import java.util.HashMap;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Properties;
import java.util.Set;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import java.util.logging.Level;

import com.hazelcast.core.HazelcastInstance;
import com.hazelcast.core.MapLoaderLifecycleSupport;
import com.hazelcast.core.MapStore;
import com.hazelcast.logging.ILogger;
import com.hazelcast.logging.Logger;
import com.sleepycat.je.CheckpointConfig;
import com.sleepycat.je.Cursor;
import com.sleepycat.je.Database;
import com.sleepycat.je.DatabaseConfig;
import com.sleepycat.je.DatabaseEntry;
import com.sleepycat.je.Environment;
import com.sleepycat.je.EnvironmentConfig;
import com.sleepycat.je.LockMode;
import com.sleepycat.je.OperationStatus;

@SuppressWarnings("unchecked")
public class BerkeleyDBStore<K, V> implements MapLoaderLifecycleSupport, MapStore<K, V>, Runnable {
  private final ILogger _logger = Logger.getLogger(BerkeleyDBStore.class.getName());

  private Database _db; 
  private static Environment _env;
  private static Map<String, Database> _dbMap = new HashMap<String, Database>();
  static {
    EnvironmentConfig envConfig = new EnvironmentConfig();
    envConfig.setAllowCreate(true);
    envConfig.setLocking(true); 
    envConfig.setSharedCache(true);
    envConfig.setTransactional(false);
    envConfig.setCachePercent(10); 
    envConfig.setConfigParam(EnvironmentConfig.LOG_FILE_MAX, "104857600"); 

    File file = new File(System.getProperty("user.dir", ".") + "/db/");
    if (!file.exists() && !file.mkdirs()) {
      throw new RuntimeException("Can not create:" + System.getProperty("user.dir", ".") + "/db/");
    }
    _env = new Environment(file, envConfig);
  }

  private int _syncinterval; 
  private ScheduledExecutorService _scheduleSync; 

  private HazelcastInstance _hazelcastInstance;
  private Properties _properties;
  private String _mapName;

  private Object entryToObject(DatabaseEntry entry) throws Exception {
    int len = entry.getSize();
    if (len == 0) {
      return null;
    } else {
      return KryoSerializer.read(entry.getData());
    }
  }

  private DatabaseEntry objectToEntry(Object object) throws Exception {
    byte[] bb = KryoSerializer.write(object);

    DatabaseEntry entry = new DatabaseEntry();
    entry.setData(bb);
    return entry;
  }

  @Override
  public void init(HazelcastInstance hazelcastInstance, Properties properties, String mapName) {
    _hazelcastInstance = hazelcastInstance;
    _properties = properties;
    _mapName = mapName;

    DatabaseConfig dbConfig = new DatabaseConfig();
    dbConfig.setAllowCreate(true);
    dbConfig.setDeferredWrite(true); //延迟写
    dbConfig.setSortedDuplicates(false);
    dbConfig.setTransactional(false);
    _db = _env.openDatabase(null, _mapName, dbConfig);
    _dbMap.put(_mapName, _db);

    if (_scheduleSync == null) {
      try {
        _syncinterval = Integer.parseInt(_properties.getProperty("syncinterval"));
      } catch (Exception e) {
        _syncinterval = 3;
        _logger.log(Level.WARNING, e.getMessage(), e);
      }
      if (_syncinterval > 0) {
        _scheduleSync = Executors.newSingleThreadScheduledExecutor(); 
        _scheduleSync.scheduleWithFixedDelay(this, 1, _syncinterval, TimeUnit.SECONDS);
      }
    }
    _logger.log(Level.INFO, this.getClass().getCanonicalName() + ":" + _mapName + ":count:" + _db.count());
    _logger.log(Level.INFO, this.getClass().getCanonicalName() + ":" + _mapName + ":inited!");
  }

  @Override
  public void destroy() {
    if (_scheduleSync != null) {
      try {
        _scheduleSync.shutdown();
      } finally {
        _scheduleSync = null;
      }
    }

    if (_db != null) {
      try {
        _db.sync();
      } catch (Throwable ex) {
        _logger.log(Level.WARNING, ex.getMessage(), ex);
      }

      try {
        _db.close();
      } catch (Throwable ex) {
        _logger.log(Level.WARNING, ex.getMessage(), ex);
      } finally {
        _db = null;
        _dbMap.remove(_mapName);
      }
      _logger.log(Level.INFO, this.getClass().getCanonicalName() + ":" + _mapName + ":count:" + _db.count());
      _logger.log(Level.INFO, this.getClass().getCanonicalName() + ":" + _mapName + ":destroyed!");
    }

    if (_dbMap.size() == 0) {
      try {
        boolean anyCleaned = false;
        while (_env.cleanLog() > 0) {
          anyCleaned = true;
        }
        if (anyCleaned) {
          CheckpointConfig force = new CheckpointConfig();
          force.setForce(true);
          _env.checkpoint(force);
        }
      } catch (Throwable ex) {
        _logger.log(Level.WARNING, ex.getMessage(), ex);
      }

      try {
        _env.close();
      } catch (Throwable ex) {
        _logger.log(Level.WARNING, ex.getMessage(), ex);
      } finally {
        _env = null;
      }
      _logger.log(Level.INFO, this.getClass().getCanonicalName() + ":BerkeleyDB Closed!");
    }

  }

  @Override
  public void run() {
    try {
      _db.sync();
    } catch (Throwable ex) {
      _logger.log(Level.SEVERE, ex.getMessage(), ex);
    }
  }

  @Override
  public V load(K key) {
    try {
      DatabaseEntry keyEntry = objectToEntry(key);
      DatabaseEntry valueEntry = new DatabaseEntry();
      OperationStatus status = _db.get(null, keyEntry, valueEntry, LockMode.DEFAULT);
      if (status == OperationStatus.SUCCESS) {
        return (V) entryToObject(valueEntry);
      } else {
        return null;
      }
    } catch (Exception e) {
      _logger.log(Level.SEVERE, e.getMessage(), e);
      return null;
    }
  }

  @Override
  public void delete(K key) {
    try {
      _db.delete(null, objectToEntry(key));
      if (_syncinterval == 0) {
        _db.sync();
      }
    } catch (Exception e) {
      _logger.log(Level.SEVERE, e.getMessage(), e);
    }
  }

  @Override
  public void deleteAll(Collection<K> keys) {
    for (K key : keys) {
      this.delete(key);
    }
    if (_syncinterval == 0) {
      _db.sync();
    }
  }

  @Override
  public void store(K key, V value) {
    try {
      DatabaseEntry keyEntry = objectToEntry(key);
      DatabaseEntry valueEntry = objectToEntry(value);
      _db.put(null, keyEntry, valueEntry);
      if (_syncinterval == 0) {
        _db.sync();
      }
    } catch (Exception e) {
      _logger.log(Level.SEVERE, e.getMessage(), e);
    }
  }

  @Override
  public void storeAll(Map<K, V> map) {
    for (Entry<K, V> entrys : map.entrySet()) {
      this.store(entrys.getKey(), entrys.getValue());
    }
    if (_syncinterval == 0) {
      _db.sync();
    }
  }

  @Override
  public Map<K, V> loadAll(Collection<K> keys) {
    Map<K, V> map = new java.util.HashMap<K, V>(keys.size());
    for (K key : keys) {
      map.put(key, this.load(key));
    }
    return map;
  }

  @Override
  public Set<K> loadAllKeys() {
    Set<K> keys = new java.util.HashSet<K>((int) _db.count());
    Cursor cursor = null;
    try {
      cursor = _db.openCursor(null, null);
      DatabaseEntry foundKey = new DatabaseEntry();
      DatabaseEntry foundData = new DatabaseEntry();

      while (cursor.getNext(foundKey, foundData, LockMode.DEFAULT) == OperationStatus.SUCCESS) {
        keys.add((K) entryToObject(foundKey));
      }
    } catch (Exception e) {
      _logger.log(Level.SEVERE, e.getMessage(), e);
    } finally {
      cursor.close();
    }

    _logger.log(Level.INFO, this.getClass().getCanonicalName() + ":" + _mapName + ":loadAllKeys:" + keys.size());

    return keys;
  }

}
</span></pre>
