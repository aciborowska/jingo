
In the test 32 clients are connected to 1 member, the member is killed -9 and restarted repeatedly to check the effect on the clients. 

clients are configured with `-Xms250M -Xmx250M`
http://54.147.27.51/~jenkins/workspace/kill-members/3.12/2019_04_08-10_40_33/member/go 

client xml config
http://54.147.27.51/~jenkins/workspace/kill-members/3.12/2019_04_08-10_40_33/member/config-hz/client-hazelcast.xml

```
<connection-strategy async-start="true" reconnect-mode="ASYNC">
<connection-retry enabled="true">
<initial-backoff-millis>1</initial-backoff-millis>
<max-backoff-millis>300</max-backoff-millis>
<fail-on-max-backoff>false</fail-on-max-backoff>
<multiplier>1</multiplier>
<jitter>0.01</jitter>
</connection-retry>
</connection-strategy>
<network>
<cluster-members> </cluster-members>
<smart-routing>true</smart-routing>
<connection-attempt-period>9000</connection-attempt-period>
<connection-attempt-limit>200</connection-attempt-limit>
</network>
```

client are configured with high connection rates specifically to test this aspect of the system
on both sides.


After 967 iteration of kill -9 and restart 
25 out of 32 client crashed with OOME producing a hprof of 0 size. 


967 iteration makes this issue quite rare. 
hprofs of 0 size are odd, and could be caused by, clients allocating very large buffers
based on possible junk info coming from the member ?


http://jenkins.hazelcast.com/view/kill/job/kill-members/11/console

http://54.147.27.51/~jenkins/workspace/kill-members/3.12/2019_04_08-10_40_33/member

/disk1/jenkins/workspace/kill-members/3.12/2019_04_08-10_40_33/member

```
[jenkins@ip-10-72-134-107 member]$ hz-errors | sort | uniq | xargs ls -lah
-rw------- 1 jenkins jenkins 0 Apr  8 16:24 ./output/HZ/HzClient10HZ/HzClient10HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:20 ./output/HZ/HzClient11HZ/HzClient11HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:22 ./output/HZ/HzClient12HZ/HzClient12HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:25 ./output/HZ/HzClient13HZ/HzClient13HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:20 ./output/HZ/HzClient14HZ/HzClient14HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:25 ./output/HZ/HzClient19HZ/HzClient19HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:25 ./output/HZ/HzClient1HZ/HzClient1HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:20 ./output/HZ/HzClient20HZ/HzClient20HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:23 ./output/HZ/HzClient21HZ/HzClient21HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:26 ./output/HZ/HzClient22HZ/HzClient22HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:21 ./output/HZ/HzClient23HZ/HzClient23HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:23 ./output/HZ/HzClient24HZ/HzClient24HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:26 ./output/HZ/HzClient25HZ/HzClient25HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:21 ./output/HZ/HzClient26HZ/HzClient26HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:23 ./output/HZ/HzClient27HZ/HzClient27HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:26 ./output/HZ/HzClient28HZ/HzClient28HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:21 ./output/HZ/HzClient29HZ/HzClient29HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:21 ./output/HZ/HzClient2HZ/HzClient2HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:24 ./output/HZ/HzClient30HZ/HzClient30HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:22 ./output/HZ/HzClient32HZ/HzClient32HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:26 ./output/HZ/HzClient4HZ/HzClient4HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:22 ./output/HZ/HzClient5HZ/HzClient5HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:24 ./output/HZ/HzClient6HZ/HzClient6HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:27 ./output/HZ/HzClient7HZ/HzClient7HZ.hprof
-rw------- 1 jenkins jenkins 0 Apr  8 16:24 ./output/HZ/HzClient9HZ/HzClient9HZ.hprof
```

GC charts.
http://54.147.27.51/~jenkins/workspace/kill-members/3.12/2019_04_08-10_40_33/member/gc.html


interestingly all client GC charts look the same as below.  
the chart is not showing any sign of memory leak, and looks good.
![image](https://user-images.githubusercontent.com/5988678/55779201-7dec7000-5aad-11e9-9090-c7d4f9e921b0.png)



