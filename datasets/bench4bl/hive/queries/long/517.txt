The commands
  hive2 -S -e "from tmp_foo select count(1)" > my_stdout.txt
and
  hive2 -S -hiveconf mapred.job.tracker=local -hiveconf mapred.local.dir=/tmp/foo -e "from tmp_foo select count(1)" > my_stdout.txt
give different results.
The former looks like:
56
and the latter looks like:
plan = /tmp/plan61908.xml
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Job running in-process (local Hadoop)
 map = 100%,  reduce =0%
 map = 100%,  reduce =100%
Ended Job = job_local_1
56