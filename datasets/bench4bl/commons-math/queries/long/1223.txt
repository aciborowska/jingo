In both MathArrays and FastMath, some computations on double are performed by firt splitting double numbers in two numbers with about 26 bits.
This splitting fails when the numbers are huge, even if they are still representable and not infinite (the limit is about 1.0e300, eight orders of magnitude below infinity).
This can be seen by computing for example



FastMath.pow(FastMath.scalb(1.0, 500), 4);



The result is NaN whereas it should be +infinity.
or by modifying test MathArraysTest.testLinearCombination1 and scaling down first array elements by FastMath.scalb(a[i], -971) and scaling up the second array elements by FastMath.scalb(b[i], +971), which should not change the results. Here the result is a loss of precision because a safety check in MathArrays.linearCombination falls back to naive implementation if the high accuracy algorithm fails.
The reason for the wrong splitting is an overflow when computing



        final int splitFactor = 0x8000001;

        final double cd       = splitFactor * d; // <--- overflow





